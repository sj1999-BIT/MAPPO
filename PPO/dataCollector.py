"""
Idea: collect data as trajectories.

1. action: next location for the end_effector
2. the actual transition is generated by the inverse kinematics.
3. only collect the left arm as its the one that actually involves passing the boxes into the bin.
4. imitiation learning for the policy to learn to scope to the boxes.
5. devise reward function. Reward the following
    1. get boxes on spade
    2. with boxes on spade, move closer to bin
    3. drop boxes into bin
"""
import numpy as np

from variables import CUR_STATES, NEXT_STATES, ACTIONS, REWARDS, L_POS_FILENAME, R_POS_FILENAME
from env import FinalEnv

from data import append_values_to_file, load_array_from_file
from controller import SinglePolicyController
from neural_networks import ValueNetwork, PolicyNetwork
from visual import display_image

from tqdm import tqdm
from PIL import Image

import os
import cv2
import torch
import shutil
import random

from neural_networks import PolicyNetwork

def collect_buffer_data(env: FinalEnv, trajectory_num = 0, total_trajectory_num=5, buffer_size=100,
                        folder_path="./data/", policy_nn_weight_path=None):
    """
    We only store up to 100 trajectories, new trajectories will replace the oldest one
    """

    if not os.path.exists(folder_path):
        os.makedirs(folder_path)

    policyController = SinglePolicyController(policy_weight_path=policy_nn_weight_path)

    # based on number of trajectories we are collecting
    for i in range(total_trajectory_num):
        trajectory_num += 1

        # replace the oldest buffer
        if trajectory_num >= buffer_size:
            trajectory_num = 0

        # In testing, a fixed sequence of randomly seeds will be set here
        env.reset()

        top_camera = env.top_camera

        # cur_timestep = 0

        # preparing folder to get all data for current trajectory

        trajectory_dir_path = os.path.join(folder_path, str(trajectory_num))

        if not os.path.exists(trajectory_dir_path):
            os.makedirs(trajectory_dir_path)
        else:
            print(f"remove oldest trajectory: {trajectory_dir_path}")
            shutil.rmtree(trajectory_dir_path)

        cur_state_img_folder_path = os.path.join(trajectory_dir_path, CUR_STATES)
        if not os.path.exists(cur_state_img_folder_path):
            os.makedirs(cur_state_img_folder_path)

        # each trajectory collect 200 transitions
        # based on the 1000 images for the demo trajectory length, now 4x sparse
        for cur_timestep in tqdm(range(200), desc="getting transitions"):

            has_box, output, reward = policyController.act(env.get_env_variables())
            if not has_box:
                break

            cur_state = env.get_RGBD_numpy(top_camera)

            append_values_to_file(output, os.path.join(trajectory_dir_path, "output.txt"))
            append_values_to_file(reward, os.path.join(trajectory_dir_path, "reward.txt"))
            print(cv2.imwrite(os.path.join(cur_state_img_folder_path, f"{cur_timestep}.png"), cur_state))

            for _ in range(env.frame_skip):
                env.step()


    # env.end_episode()
    # env.close()
    return trajectory_num


def get_TD_error(valueNet: ValueNetwork, cur_states_folder_path, reward_arr, discount_factor=0.9):
    """
    Path to all the states stored as images. We use the valueNetwork to generate the TD error.
    1. go through the states, one by one load the states according to timestep sequence
    2. generate the Value estimates
    3. TD error δₜ = rₜ + γV(sₜ₊₁) - V(sₜ)
    """

    images_filenames = sorted(os.listdir(cur_states_folder_path), key=lambda x: int(x.split('.')[0]))

    value_est_arr = []

    for i in tqdm(range(len(images_filenames)), desc="generate value est"):
        img_filename = images_filenames[i]
        img_filepath = os.path.join(cur_states_folder_path, img_filename)
        state = cv2.imread(img_filepath, cv2.IMREAD_UNCHANGED)  # make sure load all 4 channels
        # print(state.shape)
        with torch.no_grad():
            value_est = valueNet.forward(state)
            value_est_arr.append(value_est.item())

    # convert to np for efficient calculation
    # remove last reward as it will not be part of a transition
    reward_arr_np = np.array(reward_arr[:-1])
    cur_state_value_est_np = np.array(value_est_arr[:-1])
    next_state_value_est_np = np.array(value_est_arr[1:])

    return_np = reward_arr_np + discount_factor * next_state_value_est_np
    TD_error_np = return_np - cur_state_value_est_np

    return TD_error_np, return_np


def generate_PPO_training_batch(data_folder_path, pNet:PolicyNetwork ,vNet:ValueNetwork, batch_size=64):
    """
    given a datapath, containing subfolders of trajectory. We randomly pick one, it should contain a full trajectory of data.
    We need to obtain the following
        1. load the reward, action arr and states
        2. convert reward to GAE advantages
        3. generate cur_state, next_state, action transitions
        3. generate arr of random index as sample from batch size
        4. sample from the trajectory to create a sample batch to be passed for PPO update.
    """
    if not os.path.exists(data_folder_path):
        print(f"no data found at {data_folder_path}")
        return False, []

    trajectory_folder_name_list = os.listdir(data_folder_path)

    # pick a random trajectory
    cur_trajectory_folder_name = random.choice(trajectory_folder_name_list)

    # make sure we have enough states to pick batch size transitions
    # -1 as transitions will be one lesser than timesteps

    trajectory_data_path = os.path.join(data_folder_path, cur_trajectory_folder_name)

    cur_states_folder_path = os.path.join(trajectory_data_path, CUR_STATES)

    trajectory_len = len(os.listdir(cur_states_folder_path))-1
    if trajectory_len < batch_size:
        print(f"trajectory {cur_trajectory_folder_name} is too short, only {trajectory_len} "
              f"transitions for batch size {batch_size}")
        return False, []

    reward_arr = load_array_from_file(os.path.join(trajectory_data_path, "reward.txt"))

    # need to reshape output is sizes of 24
    output_arr = load_array_from_file(os.path.join(trajectory_data_path, "output.txt"))
    output_arr_np = np.array(output_arr).reshape(-1, 24)

    # calculate the td_error based on current valueNet
    td_error_np, return_np = get_TD_error(vNet, cur_states_folder_path=cur_states_folder_path,
                               reward_arr=reward_arr, discount_factor=0.9)

    # now we need to select batch size sample fo transitions
    # should have cur_state, next_state, output, td_error, old_prob

    batch_random_indices = random.sample(range(td_error_np.size-1), batch_size)

    batch_image_filename_list = np.array(os.listdir(cur_states_folder_path))[batch_random_indices]

    # 64, 480, 640, 4
    batch_state_arr = []
    for i in tqdm(range(batch_size), desc="loading batch images"):
        image_filename = batch_image_filename_list[i]
        img_filepath = os.path.join(cur_states_folder_path, image_filename)
        img = cv2.imread(img_filepath, cv2.IMREAD_UNCHANGED)
        batch_state_arr.append(img)

    batch_state_np = np.array(batch_state_arr)

    batch_output_np = output_arr_np[batch_random_indices]

    print(batch_output_np.shape)

    batch_log_prob_tensor = pNet.get_action_probability(batch_state_np, batch_output_np)


    # state_tensor = torch.tensor(state_arr, dtype=torch.float32)

    print(batch_image_filename_list)

    return {
        'states': batch_state_np,
        'actions': batch_output_np,
        'old_log_probs': batch_log_prob_tensor,
        'advantages': td_error_np[batch_random_indices],
        'returns': return_np[batch_random_indices]
    }


def get_reward(env, weight_path, total_time_step=200):
    """
    Test the effectiveness of the pNet
    """

    # env = FinalEnv()
    env.reset()

    controller = SinglePolicyController(policy_weight_path=weight_path)

    reward = 0

    for i in tqdm(range(total_time_step), desc="testing the policy performance"):
        has_box, output, cur_reward = controller.act(env.get_env_variables())
        if not has_box:
            break
        reward += cur_reward

    print(f"current total reward {reward}")

    return reward





if __name__ == "__main__":
    data_folder_path = "./data/"

    # define variables
    input_height = 224
    input_width = 300
    output_dim = 512
    action_dim = 24
    interval = 1
    action_bounds = [-1, 1]

    # self.rgbd_nn = RGBDNetwork(input_height, input_width, output_dim)
    policyNet = PolicyNetwork(n_states=output_dim,
                         n_actions=action_dim,
                         input_height=input_height,
                         input_width=input_width,
                         action_bounds=action_bounds)
    valueNet = ValueNetwork()

    batch_dict = generate_PPO_training_batch(data_folder_path, policyNet, valueNet, batch_size=64)

    print(policyNet.get_loss(batch_dict))


