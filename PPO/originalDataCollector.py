"""
Idea: collect data as trajectories.

1. action: next location for the end_effector
2. the actual transition is generated by the inverse kinematics.
3. only collect the left arm as its the one that actually involves passing the boxes into the bin.
4. imitiation learning for the policy to learn to scope to the boxes.
5. devise reward function. Reward the following
    1. get boxes on spade
    2. with boxes on spade, move closer to bin
    3. drop boxes into bin
"""
import numpy as np

from variables import CUR_STATES, NEXT_STATES, ACTIONS, REWARDS, L_POS_FILENAME, R_POS_FILENAME
from env import FinalEnv

from data import append_values_to_file, load_array_from_file
from controller import SinglePolicyController
from visual import display_image

from tqdm import tqdm
from PIL import Image

import os
import cv2
import random

from neural_networks import RGBDNetwork, PolicyNetwork


# class DataCollector:
#     def __init__(self):
#         """
#         Store trajectory as a dictionary, mapping the key string to arrays.
#         1. cur_state: RGBD from the arm camera current state and location of end_effector at current state.
#         2. next_state: RBGD from the arm camera current state and location of end_effector at next state.
#         3. action: location for the end_effector to move to
#         4. reward: reward for the next state.
#         """

def collect_data(env: FinalEnv, total_trajectory_num=5, folder_path="./data/", policy_nn_weight_path=None):

    if not os.path.exists(folder_path):
        os.makedirs(folder_path)

    def create_trajectory_dir(folder_path):
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)
        if not os.path.exists(os.path.join(folder_path, CUR_STATES)):
            os.makedirs(os.path.join(folder_path, CUR_STATES))

    def get_robotic_position_and_rotation():
        end_effector_poses = env.get_end_effector_poses()

        left_robot_arr, right_robot_arr = [], []

        left_robot_arr.extend(end_effector_poses['left_robot']['position'])

        for subarr in end_effector_poses['left_robot']['rotation_matrix']:
            left_robot_arr.extend(subarr)

        right_robot_arr.extend(end_effector_poses['right_robot']['position'])

        for subarr in end_effector_poses['right_robot']['rotation_matrix']:
            right_robot_arr.extend(subarr)

        # print(f"{len(left_robot_arr)}, {len(right_robot_arr)}")

        return left_robot_arr, right_robot_arr

    policyController = SinglePolicyController(policy_weight_path=policy_nn_weight_path)
    is_new_trajectory = False

    trajectory_num = 0

    # based on number of trajectories we are collecting
    while trajectory_num < total_trajectory_num:

        # In testing, a fixed sequence of randomly seeds will be set here
        env.reset()

        left_camera = env.left_ee_camera
        right_camera = env.right_ee_camera
        top_camera = env.top_camera

        phase = 0
        cur_timestep = 0


        pbar = tqdm(total=env.global_max_steps, desc="getting transitions")

        while env.local_total_timesteps < env.local_max_steps:

            if phase == 0 and not is_new_trajectory:
                cur_timestep = 0
                is_new_trajectory = True
                print(f"collecting new trajectory {trajectory_num}")

                trajectory_dir_path = os.path.join(folder_path, str(trajectory_num))
                # create new folder to store data for current trajectory
                create_trajectory_dir(trajectory_dir_path)
                cur_state_img_folder_path = os.path.join(trajectory_dir_path, CUR_STATES)
                trajectory_num += 1

            has_box, action_left, action_right = policyController.act(env.get_env_variables())
            if not has_box:
                break

            if phase == 1 or phase == 2:
                # we do not care for phase 0 or when there is no action to be collected

                print(phase)
                # cur_state = np.concatenate((env.get_RGBD_numpy(right_camera), env.get_RGBD_numpy(top_camera)),
                #                            axis=1)

                cur_state = env.get_RGBD_numpy(top_camera)

                # print(cur_state.shape)

                left_robot_arr, right_robot_arr = get_robotic_position_and_rotation()

                # print(right_robot_arr)

                append_values_to_file(right_robot_arr, os.path.join(trajectory_dir_path, "right_arm_positions.txt"))

                append_values_to_file(left_robot_arr, os.path.join(trajectory_dir_path, "left_arm_positions.txt"))



                print(cv2.imwrite(os.path.join(cur_state_img_folder_path, f"{cur_timestep}.png"), cur_state))

                cur_timestep += 1

                pbar.update(1)

            for _ in range(env.frame_skip):
                env.step()

            # after stepped, we collect the next state
            if phase > 2 and is_new_trajectory:
                cur_timestep = 0
                is_new_trajectory = False
                if trajectory_num > total_trajectory_num:
                    break

    env.end_episode()
    env.close()


def collect_buffer_data(env: FinalEnv, total_trajectory_num=5, folder_path="./data/", policy_nn_weight_path=None):

    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
    else:
        os.remove(folder_path)

    def create_trajectory_dir(folder_path):
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)
        if not os.path.exists(os.path.join(folder_path, CUR_STATES)):
            os.makedirs(os.path.join(folder_path, CUR_STATES))

    def get_robotic_position_and_rotation():
        end_effector_poses = env.get_end_effector_poses()

        left_robot_arr, right_robot_arr = [], []

        left_robot_arr.extend(end_effector_poses['left_robot']['position'])

        for subarr in end_effector_poses['left_robot']['rotation_matrix']:
            left_robot_arr.extend(subarr)

        right_robot_arr.extend(end_effector_poses['right_robot']['position'])

        for subarr in end_effector_poses['right_robot']['rotation_matrix']:
            right_robot_arr.extend(subarr)

        # print(f"{len(left_robot_arr)}, {len(right_robot_arr)}")

        return left_robot_arr, right_robot_arr

    policyController = SinglePolicyController(policy_weight_path=policy_nn_weight_path)
    is_new_trajectory = False

    trajectory_num = 0

    # based on number of trajectories we are collecting
    while trajectory_num < total_trajectory_num:

        # In testing, a fixed sequence of randomly seeds will be set here
        env.reset()

        left_camera = env.left_ee_camera
        right_camera = env.right_ee_camera
        top_camera = env.top_camera

        cur_timestep = 0

        pbar = tqdm(total=env.global_max_steps, desc="getting transitions")

        while env.local_total_timesteps < env.local_max_steps:



            if phase == 0 and not is_new_trajectory:
                cur_timestep = 0
                is_new_trajectory = True
                print(f"collecting new trajectory {trajectory_num}")

                trajectory_dir_path = os.path.join(folder_path, str(trajectory_num))
                # create new folder to store data for current trajectory
                create_trajectory_dir(trajectory_dir_path)
                cur_state_img_folder_path = os.path.join(trajectory_dir_path, CUR_STATES)
                trajectory_num += 1

            has_box, action_left, action_right = policyController.act(env.get_env_variables())
            if not has_box:
                break

            if phase == 1 or phase == 2:
                # we do not care for phase 0 or when there is no action to be collected

                print(phase)
                # cur_state = np.concatenate((env.get_RGBD_numpy(right_camera), env.get_RGBD_numpy(top_camera)),
                #                            axis=1)

                cur_state = env.get_RGBD_numpy(top_camera)

                # print(cur_state.shape)

                left_robot_arr, right_robot_arr = get_robotic_position_and_rotation()

                # print(right_robot_arr)

                append_values_to_file(right_robot_arr, os.path.join(trajectory_dir_path, "right_arm_positions.txt"))

                append_values_to_file(left_robot_arr, os.path.join(trajectory_dir_path, "left_arm_positions.txt"))



                print(cv2.imwrite(os.path.join(cur_state_img_folder_path, f"{cur_timestep}.png"), cur_state))

                cur_timestep += 1

                pbar.update(1)

            for _ in range(env.frame_skip):
                env.step()

            # after stepped, we collect the next state
            if phase > 2 and is_new_trajectory:
                cur_timestep = 0
                is_new_trajectory = False
                if trajectory_num > total_trajectory_num:
                    break

    env.end_episode()
    env.close()


def get_imitation_training_data(data_dir, batch_size=64, interval=1):
    """
    data_dir contains trajectory_dir
    each trajectory_dir contains a cur_states folder for all the RGBD and 2 txt files representing the left and right arms
    create a training batch randomly sampled from the data_dir

    the agent can take in a single RGBD, or a sequence of rgbd, use intervals
    """

    batch = []

    if not os.path.exists(data_dir):
        print(f"data directory {data_dir} does not exist")
        return False, batch,

    # get all trajectory directories
    data_dir_items = os.listdir(data_dir)
    trajectory_dir_path_arr = [os.path.join(data_dir, item) for item in data_dir_items
                           if os.path.isdir(os.path.join(data_dir, item))]

    if len(trajectory_dir_path_arr) == 0:
        print(f"no trajectory data collected at data directory {data_dir}")
        return False, batch,

    for _ in range(batch_size):
        # pick a random trajectory directory
        trajectory_dir_path = random.choice(trajectory_dir_path_arr)

        # folder contains rgbd files for current trajectory
        states_folder_path = os.path.join(trajectory_dir_path, CUR_STATES)
        # array containing all states file path
        states_file_path_arr = os.listdir(states_folder_path)
        # Sort in-place, all files must be in sequence
        states_file_path_arr.sort()

        # load the actions from the file as a single array
        right_arm_actions = load_array_from_file(os.path.join(trajectory_dir_path, R_POS_FILENAME))
        # reshape it, each action represents the position and rotation of the end_effector of right arm, 12dim
        right_arm_actions = np.array(right_arm_actions).reshape(-1, 12)

        # randomly choose one for current transition
        # cannot choose with the last folder of intervals as we need the action after interval of current timestep
        cur_state_filename = random.choice(states_file_path_arr[:-interval])

        # use the timestep to get the correct action
        time_step = int(cur_state_filename.split(".")[0])
        # print(time_step)

        # collect interval number of foldername to be combined to form the inputs for the neural network
        state_arr = []

        for i in range(interval):
            state_arr.append(os.path.join(states_folder_path, states_file_path_arr[time_step]))
            time_step += 1

        # choose the next action for current transition
        right_arm_action = right_arm_actions[time_step]

        batch.append({CUR_STATES: state_arr, ACTIONS: right_arm_action})

    # print(batch)

    # current batch states are not loaded yet, need to load the RGBD images into a single np array
    for transition in batch:
        state_folder_path_arr = transition[CUR_STATES]
        final_state_npy = None
        for state_folder_path in state_folder_path_arr:
            rgbd_arr = Image.open(state_folder_path)
            if final_state_npy is None:
                final_state_npy = np.array(rgbd_arr)
            else:
                final_state_npy = np.concatenate((final_state_npy, rgbd_arr), axis=1)

        transition[CUR_STATES] = final_state_npy
        # cv2.imshow("test", final_state_npy)
        # cv2.waitKey(0)
        #
        # print(final_state_npy.shape)

    print(f"batch data of size {batch_size} is collected")
    return True, batch






if __name__ == "__main__":
    # np.random.seed(0)
    # env = FinalEnv()

    # collect_data(env, total_trajectory_num=200)
    success, batch = get_imitation_training_data("./data/", interval=1)

    # input_height = len(batch[0][CUR_STATES])
    # input_width = len(batch[0][CUR_STATES][0])

    # print(f'{input_height},{input_width}')

    # rgbd net processed batch transitions into inputs for policy network
    rgbd_net = RGBDNetwork(input_height=224, input_width=600, output_dim=512)
    batch_policy_inputs = rgbd_net.process_inputs(batch)

    # action bound not considered for now
    p_net = PolicyNetwork(n_states=524, n_actions=12, action_bounds=[-1, 1])
    output_actions, log_probs = p_net.sample_or_likelihood(batch_policy_inputs)
    print(output_actions)


